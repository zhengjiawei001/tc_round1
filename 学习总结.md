主要从数据增强、模型训练方式、模型预训练技巧、模型魔改、模型融合进行简单总结

数据增强(自己总结不了这么精辟的，虽然也是这么操作的，但是没有闭包数据增强的概念<0_0>)

参考电费战士的分享(https://zhuanlan.zhihu.com/p/360705398)

1、对偶数据增强，即Q1-Q2 pair变成Q2-Q1 pair；

2、闭包数据增强，即Q1-Q2=1，Q2-Q3=1，则Q1-Q3=1；

注意：数据增强时要保证正负样本比例，与原始分布一致，否则无效果甚至导致效果变差。

模型训练方式
1、ESIM,尝试的第一个模型，但是没有上0.9,之后换bert了
2、使用MLM进行预训练，然后使用BertForSequenceClassification进行微调


模型预训练技巧
1、全词mask(wwm)，通过统计常在一起token的概率，找到全词；

2、n-gram(掩码)，但是看到前排大佬使用了n-gram混合掩码，采用1、2、3-gram混合掩码；

3、pair对数据与单句数据一起预训练；

4、使用开源模型的权重进行模型初始化(不加载开源模型的权重loss收敛的速度慢，同样效果稍微差点)

魔改方案(好像也没有魔改，从一开始就使用bert了，之后同时使用了nezha,但是都是没有魔改，直接拿来使用了)：

1、BERT模型+cls
2、nezha模型+cls
3、对抗训练FGM和PGD，(上次比赛的使用的时候，没有起效果,因为emb_name没有修改过来，
这次虽然修改过来了，但是没有调参，然后实验效果是fgm效果好，复赛修正过来了，
PGD效果更好，但是耗时太久)

4、在复赛中也添加了 Multi-Dropout方案

前排大佬的模型魔改方案：
1、BERT模型对Sequence Output求mean pooling与cls拼接，或者加和；

2、Multi-Dropout，详细介绍

3、BERT Embedding + Dense 与cls拼接；

4、BERT 多层cls拼接或加和；

5、BERT + GRU；

6、BERT + HighWay；

7、对抗学习FGM和PGD，PGD效果更好，但是耗时太久；

8、加统计特征。

注意：目前线上使用了第1、2、4、7。


模型融合
模型融合讲究和而不同，因此我们这里使用了多种模型进行融合。
(因为设备问题，一直没有考虑过large模型
之前一直在致力于提高单模效果，也一直在各种mask上下功夫，但是在最后一两天，
朋友提醒我，模型融合提升效果很明显，才临时拼凑了4种模型进行融合)

1、BERT

2、NeZha,学习链接:https://zhuanlan.zhihu.com/p/100044919

采用了nezha和bert分别ngram和wwm掩码策略的模型(4个模型融合,分数比较低,有很大的提升空间)